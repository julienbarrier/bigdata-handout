\documentclass[a4paper]{tufte-book}
\usepackage[utf8]{inputenc}
\hypersetup{colorlinks}% uncomment this line if you prefer colored hyperlinks (e.g., for onscreen viewing)

\title{Big Data, Statistical Learning}
\author{Julien Barrier}
\publisher{ESPCI Paris}
\newcommand{\thetitle}{Introduction to Big Data}
\newcommand{\theauthor}{Julien Barrier --- class of 2018}
\newcommand{\pc}{ESPCI Paris}
\newcommand{\thesubtitle}{Statistical \& Machine Learning}

\usepackage{microtype}
\usepackage{textcase}
\usepackage{booktabs}
\usepackage{tabularx}
\newcolumntype{R}{>{\raggedleft\arraybackslash}X}

\usepackage{graphicx}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
\graphicspath{{graphics/}}

\usepackage{fancyvrb}
\fvset{fontsize=\normalsize}

\newcommand{\hangp}[1]{\makebox[0pt][r]{(}#1\makebox[0pt][l]{)}}

\newcommand{\hangstar}{\makebox[0pt][l]{*}}

\usepackage{xspace}

\newcommand{\monthyear}{%
\ifcase\month\or January\or February\or March\or April\or May\or June\or
July\or August\or September\or October\or November\or
December\fi\space\number\year
}

\newcommand{\openepigraph}[2]{%
%\sffamily\fontsize{14}{16}\selectfont
\begin{fullwidth}
\sffamily\large
\begin{doublespace}
\noindent\allcaps{#1}\\% epigraph
\noindent\allcaps{#2}% author
\end{doublespace}
\end{fullwidth}
}

\newcommand{\blankpage}{\newpage\hbox{}\thispagestyle{empty}\newpage}

\usepackage{units}
\usepackage{siunitx}
\usepackage{stmaryrd}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{mathrsfs}

\newcommand{\measure}[3]{#1/#2$\times$\unit[#3]{pc}}

\newcommand{\hlred}[1]{\textcolor{Maroon}{#1}}% prints in red
\newcommand{\hangleft}[1]{\makebox[0pt][r]{#1}}
\newcommand{\hairsp}{\hspace{1pt}}% hair space
\newcommand{\hquad}{\hskip0.5em\relax}% half quad space
\newcommand{\TODO}{\textcolor{red}{\bf TODO!}\xspace}
\newcommand{\ie}{\textit{i.\hairsp{}e.}\xspace}
\newcommand{\eg}{\textit{e.\hairsp{}g.}\xspace}
\newcommand{\na}{\quad--}% used in tables for N/A cells
\providecommand{\XeLaTeX}{X\lower.5ex\hbox{\kern-0.15em\reflectbox{E}}\kern-0.1em\LaTeX}
\newcommand{\tXeLaTeX}{\XeLaTeX\index{XeLaTeX@\protect\XeLaTeX}}
% \index{\texttt{\textbackslash xyz}@\hangleft{\texttt{\textbackslash}}\texttt{xyz}}
\newcommand{\tuftebs}{\symbol{'134}}% a backslash in tt type in OT1/T1
\newcommand{\doccmdnoindex}[2][]{\texttt{\tuftebs#2}}% command name -- adds backslash automatically (and doesn't add cmd to the index)
\newcommand{\doccmddef}[2][]{%
\hlred{\texttt{\tuftebs#2}}\label{cmd:#2}%
\ifthenelse{\isempty{#1}}%
{% add the command to the index
    \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2}}% command name
}%
{% add the command and package to the index
    \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2} (\texttt{#1} package)}% command name
    \index{#1 package@\texttt{#1} package}\index{packages!#1@\texttt{#1}}% package name
}%
}% command name -- adds backslash automatically
\newcommand{\doccmd}[2][]{%
\texttt{\tuftebs#2}%
\ifthenelse{\isempty{#1}}%
{% add the command to the index
    \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2}}% command name
}%
{% add the command and package to the index
    \index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2} (\texttt{#1} package)}% command name
    \index{#1 package@\texttt{#1} package}\index{packages!#1@\texttt{#1}}% package name
}%
}% command name -- adds backslash automatically
\newcommand{\docopt}[1]{\ensuremath{\langle}\textrm{\textit{#1}}\ensuremath{\rangle}}% optional command argument
\newcommand{\docarg}[1]{\textrm{\textit{#1}}}% (required) command argument
\newenvironment{docspec}{\begin{quotation}\ttfamily\parskip0pt\parindent0pt\ignorespaces}{\end{quotation}}% command specification environment
\newcommand{\docenv}[1]{\texttt{#1}\index{#1 environment@\texttt{#1} environment}\index{environments!#1@\texttt{#1}}}% environment name
\newcommand{\docenvdef}[1]{\hlred{\texttt{#1}}\label{env:#1}\index{#1 environment@\texttt{#1} environment}\index{environments!#1@\texttt{#1}}}% environment name
\newcommand{\docpkg}[1]{\texttt{#1}\index{#1 package@\texttt{#1} package}\index{packages!#1@\texttt{#1}}}% package name
\newcommand{\doccls}[1]{\texttt{#1}}% document class name
\newcommand{\docclsopt}[1]{\texttt{#1}\index{#1 class option@\texttt{#1} class option}\index{class options!#1@\texttt{#1}}}% document class option name
\newcommand{\docclsoptdef}[1]{\hlred{\texttt{#1}}\label{clsopt:#1}\index{#1 class option@\texttt{#1} class option}\index{class options!#1@\texttt{#1}}}% document class option name defined
\newcommand{\docmsg}[2]{\bigskip\begin{fullwidth}\noindent\ttfamily#1\end{fullwidth}\medskip\par\noindent#2}
\newcommand{\docfilehook}[2]{\texttt{#1}\index{file hooks!#2}\index{#1@\texttt{#1}}}
\newcommand{\doccounter}[1]{\texttt{#1}\index{#1 counter@\texttt{#1} counter}}

% Generates the index
\usepackage{makeidx}
\makeindex

\usepackage{titlesec,titletoc}
\usepackage{multirow}

\begin{document}
\frontmatter

\thispagestyle{empty}
\begin{fullwidth}
    \setlength{\parindent}{0pt}
    \begin{center}
        \fontsize{24}{24}\selectfont\textit{
            \includegraphics*[width=2.6in]{ESPCI_baseline_couleur}
        }
    \end{center}
    \vspace{3in}\fontsize{36}{54}\selectfont\thetitle

    \vspace{0.125in}\fontsize{18}{18}\selectfont\thesubtitle

    \vfill\fontsize{14}{14}\selectfont\textit{\theauthor}
\end{fullwidth}

\newpage

\cleardoublepage
\chapter*{Introduction}

I have started writing this handout from the notes I have taken from Olivier Rivoire's course on
Big Data and Statistical learning at \pc{} from May to April 2018. 
Please be considerate if some mistakes crop up in this work.

\emph{Julien}
\vspace{1cm}

Some book reading is advised during the course, particularly:

\begin{itemize}
    \item \emph{The Elements of Statistical Learning}, T.~Hastie, R.~Tibshirani and J.~Friedman, Springer Series in Statistics, 2008;
    \item \emph{Information Theory, Inference, and Learning Algorithms}, D.J.C.~MacKay, Cambridge University Press, 2003.
\end{itemize}

\vspace{1cm}

\textbf{Dr Olivier Rivoire}\\
Center for Interdisciplinary Research in Biology (CIRB)\\
Collège de France\\
olivier.rivoire@college-de-france.fr\\

\section*{Applications}

There are plenty of applications for Big Data problems. A few examples may be given:
\begin{description}
    \item[Post] learn + identify digits on enveloppes
    \item[Biology] DNA sequencing
    \item[IT] Face recognition
    \item[etc.]
\end{description}

Big Data is an issue of growing importance. As engineers, we may be familiar with such concepts.
\section*{Idea of marchine learning} 

The main idea of machine learning is to find models to give prediction of input data.
In facts, Big Data models are deduced from a training batch of N input-output
data, on which programs train to generalise models.
The deduced model \emph{input i} $\rightarrow$ \emph{output i} can then be
generalised to give prediction from a random input, as long as it relates to 
the training batch.



Analyticall, let's start with a collection of $x$ and $y$ data, where $x$ stands
for the input data and $y$ is the vector of the output data.
Each sample is going to have multiple dimensions, therefore we may use an
algebraic model. Let $N$ be the number of samples used and p the dimension of
each $x$ data. We may write x as an $N,p$ matrix and $y$ as a vector of $p$
dimensions.

We now N samples of p dimensions $x_{ij}$ associated with the N output data $y_i$.

From now on there are two possible cases: $y_i$ can be known or unknown.
In the fist case ($y_i$ known), the problem is said to be \emph{supervised}.
Hence we may work with a finite discrete set of data: $y_i = 1, \cdots, K$.
This problem is called categorical, and we can solve it with 
\emph{classification}.
We may also work with an infinite set of numbers: $y_i \in \mathbb{R}$. This
problem is called quantitative, and we can solve it with \emph{regression}.

The second case ($y_i$ unknown) is said to be \emph{unsupervised} and can be
solved via \emph{clustering} or \emph{dimension reduction} methods.

\section*{Deep learning}

In the past few years, there have been huge progress in the \emph{deep learning}
approach. It is based on so-called neurol networks, that are models inspired
by the brain operation.

People are trying to understand how to train these networks. It has had
remarkable outcomes in image recognition, social network filtering, medical
diagnoses, etc.

Deep learning is based on hidden layers, placed inbetween input and output layers
, that are trained to find correlations and mathematical models.

The goal of this course is to explain whate these objects are, how do they work,
and put it in relation with state of the art research.

What kind of open problems are there? How do neural networks operate? What are
their unsuperised learning behaviour?


\tableofcontents\thispagestyle{empty}

\mainmatter

\chapter{Least square regression, from small to big data}
\label{ch:least-square}

\subsection{Linear Regression}

\paragraph{Least square regression at one dimension}

Let $p =1$. If we work with N points, then $i=1,\cdots,N$, and we work with a
set of data $(x_i,y_i)$.

The goal here is to make a prediction of what the $y$ data should be when $x$ is
given.

The simplest possible model is the linear regression given by the equation 
\ref{linear1D}.
\begin{equation}
    y=\alpha + \beta x.
    \label{linear1D}
\end{equation}
Here, the main issue is to get the best $\alpha$ and $\beta$ for a particular set
of data. To know what the best choice is, we may define a cost function, that
returns a number representing how well the regression permorms. In neural network
problems, the cost fuction return number is associated with how well the neural
network performs in mapping training examples to the correct output.

There are several choices that can be made to define the cost function. At one
dimension, the simplest choice is the sum of squared residuals, defined in
equation \ref{SSR}, usually shortened as SSR.

\begin{equation}
    l(\alpha,\beta) = \frac{1}{N} \sum_{i=1}^N (y_i - \alpha - \beta x_i )^2
    \label{SSR}
\end{equation} 

Figure \ref{fig1} illustrate a simple geometrical interpretation of what the SSR
is. Actually, the lower $\epsilon_i$, the better the fit.

\begin{marginfigure}
\TODO
\caption{geometrical intepretation of the SSR, where $\epsilon_i$ is given by the relation: $\epsilon_i^2 = (y_i - \alpha - \beta x_i)^2$}
\label{fig1}
\end{marginfigure}

For all we have done up to now, we never have never worked with big data. We
need $p$ large enough to consider this as a real big data issue.

If we’re looking at a hundreds or thousands pixels picture composed of hundreds, 
$p$ will be large in comparison with $N$. That is a full statistics problem.

Currently, $p=1$ is small data, but all we did there has been a correct
introduction to clearly understand big data problems.

Striking a good fit necessitates finding the best $\alpha$ and the best $\beta$.
For this, we may look at the optimum, defined as the points where the derivative
of $l$ versus $\alpha$ and $\beta$ vanishes. This is given by equations
\ref{derlalpha} and \ref{derlbeta}.

\begin{eqnarray}
    \frac{\partial l}{\partial \alpha} & = & - \frac{1}{N} \sum_{i=1}^N (y_i - \alpha - \beta x_i) = 0
    \label{derlalpha}\\
    \frac{\partial l}{\partial \beta} & =&  -\frac{1}{N} \sum_{i=1}^N x_i (y_i - \alpha -  \beta x_i) = 0
    \label{derlbeta}
\end{eqnarray}

To solve this set of equations, we require a substitution for $x$ and $y$.

Let’s define\footnote{We must keep in mind that $\overline{x^2} \neq \bar{x}^2$.}:
\begin{eqnarray}
    \bar{x} & = & \frac{1}{N} \sum_{i=1}^N x_i\\
    \bar{y} & = & \frac{1}{N} \sum_{i=1}^N y_i\\
    \overline{xy} & =&  \frac{1}{N} \sum_{i=1}^{N} x_i y_i\\
    \overline{x^2} & =&  \frac{1}{N} \sum_{i=1}^N x_i^2
\end{eqnarray}

Thus, equations \ref{derlalpha} and \ref{derlbeta} can be reduced as:
\begin{eqnarray}
    \frac{\partial l}{\partial \alpha} &=& - (\bar{y} - \alpha - \beta \bar{x})
    \label{deralpha2}\\
    \frac{\partial l}{\partial \beta} &=& - (\overline{xy} - \alpha \bar{x} - \beta \overline{x^2})
    \label{derbeta2}
\end{eqnarray}

This yields to:

\begin{eqnarray}
    \alpha & = & \bar{y} + \beta \bar{x}\\
    \overline{xy} & = &  -\bar{y}\bar{x} - \beta \bar{x}^2 - \beta \overline{x^2}
\end{eqnarray}


There we get $\hat{\beta} = \frac{\bar{xy} - \bar{x}\bar{y}}{\bar{x^2} - \bar{x}^2} = \frac{cov(x,y)}{cov(x,x)} = \frac{cov(x,y)}{var(x)}$

$\hat{\alpha} = \bar{y} - \hat{\beta}\bar{x}$

NB: $cov(x,y) = \bar{xy} - \bar{x}\bar{y} = \bar{(x-\bar{x})(y-\bar{y})$

$var(x) = com (x,x)$

$std(x) = \sqrt{var(x)}$

Pearson coefficient $\mathcal{R}$ is defined by:
$\mathcal{R} = \frac{cov(x,y)}{std(x) std(y)}$

$\mathcal{R}^2 = 1 - \frac{\hat{l}}{var(y)}$

This quantity relates to the quality of the fit. $\mathcal{R}$ is comprised between 0 and 1. 1 corresponds to a good fit, 0, to a very bad one.

Illustration: M & H Bornstein, Nature 1976. in general, we’re looking at models where $\beta = 0$. In this case, the cost function l would be the sum of the square distance to the line.
In the second case, much smaller l.

Data: speed at which people are working in different cities. They estimated the speed, and made a regression versus the population size of the city. The claim of the paper is to say there’s a pretty good correlation of the speed people are walking and the size of the town. This is not a linear fit, because logarithmic scale in the population.
In practice, we plot x versus y, it is usually better to transform x by log(x) to find the best linear regression. Formally, it is the same formula.

We’re now moving to p>1, and full data, that means that p<<N.
If we consider the previous example of the linear regression, let’s consider other relevant parameters, for example the average heigh of people, sex, etc., let’s examine many potential predictors. We need to generalise the same thing, where each input is a vector of dimension p:

$x_1, … , x_p$.

Now we have $x_{i,j}$ where $I$ is the sample, from 1 to N, and j is the dimension, from 1 to p.

There the generalisation $\hat{y_i} = \hat{\alpha} + \hat{\beta}x_i$ is now:

\begin{equation}
\hat{y_i} = \hat{\alpha} + \sum_{j=1}^p \hat{\beta_j} x_{ij}
\end{equation}

If we have this key figure, we can always add 1 in the x vector, as the p+1 coordinate. Thus we can assume that $\alpha$ vanishes. In other terms, we
can always redefine the data so that $\alpha$ vanishes. We can also rescale the variable, by removing the mean:

\begin{eqnarray}
x’&=& x-\bar{x}\\
y’&=& y-\bar{y}
\end{eqnarray}

Therefore, let $\hat{y_i}$ be written as $\hat{y_i} = X \hat{\beta}$, that is a much more convenient way to write it.

That are just restrictions of the problems that help us to compute it.

Let $l(\beta)$ be the cross-function.

\begin{equation}
l(\beta) = \frac{1}{N} \sum_{i=1}^N \left( y_i - \sum_{j=1}^p \beta_j x_{ij} \right)^2
\end{equation}

Let Z be a vector of which the components $z_i$ are defined as: $\sum_{i=1}^N z_i^2 = ||Z||^2 = Z^TZ.$

Therefore we can write the cross-function as:

\begin{equation}
l(\beta) = \frac{1}{N} (Y-X\beta)^T(Y-X\beta).
\end{equation}
The advantage of this form is that is easily differentiates with $\beta$:

$\frac{\partial l}{\partial \beta} =  -\frac{Z}{N} X^T(Y-X\beta) =0$ to get the extremum.

NB: T c’est la notation américaine, on met le signe transposée après la transposée, et non pas avant comme en français. Donc X^TX est la transposée de X fois X.

If we need to solve $X^TY = X^TX\beta$ (the previous equation), we get:

\begin{equation}
\hat{\beta} = (X^TX)^{-1} X^TY = C^{-1} X^TY
\end{equation}
with $C = X^TX$

\begin{equation}
C_{jk} = \sum_{i=1}^N x_{ij} x_{ik}
\end{equation}

exercise, try to come with p=1 and…

All the geometry consists in fitting with an hyperplane. We can try to draw a figure of an hyperplane as an example in the handout.
essentially, we are solving a system of equations. We have to consider the number of variables adapted to the number of equation that we get. If we have not enough equations, for example p too small, the system is underrmined. We cannot reduce it and do not have a single solution.
Actually, when $p>N$, we can solve this problem with $\hat{l}=0$. This is a situation where there are more parameters than there are equaltions. It’s easy to solve. The solutions consists in overfitting.

For instance if we have 100 parameters and 10 equations, we can never manage to get any result. We could find easy solutions, but this will overfit. (like trivial salutions).
At this stage, the system cannot be inverted.

Even if p is comparable to N, it will still be true that we should be careful: there are several reasons. We have very big matrixes, inverted it could be tricky. It will take a lot of time, we could have problems of mathematical accuracy. Actually, it doesn’t make sense to have too much parameters in the problem.

This is where people use big data. We have these problems in many situations. If we want to discriminate between different datasets., p can be very large with big data problems.

So, what do we do now?
In general, we want to know what are the most interesting parameters. At the end, we can predict the issue with one or two parameters. Even if we start with a lot of data, we want to find what are the parameterts, the dimensions important in the problem we are working on.

Two advantages:
\begin{description}
\item[Interpretation]
With less parameters, we can estimate them with much more accuracy than if we have more.
In general, it’s easier and more accurate to estimate things on a condensed set of parameters than on a large set.
The issue is: how do you compromise: having enough parameters to have a good enough estimation of the problem, without having too much and loosing precision. Enough information VS enough precision.
\end{description}

Let $\tilde{l}(\beta)$ be defined as:
\begin{equation}
\tilde{l}(\beta) = l(\beta) + \lambda ||\beta||_q
\end{equation}
With $||\beta||_0 = #(\beta_j \neq 0)$ (cardinal)

We look for $min_\beta l(\beta)$ given $||\beta||_0 \leq C$

With $\beta = [0, \cdots, \beta_i, 0, \cdots, 0]$
There ain’t any good numerical solution.

There’s always a compromise between what we are able to optimise efficiently, and what is possible to optimise. A version of the problem that is easy to solve is:

\begin{equation}
min_\beta l(\beta)  given ||\beta||_2 \leq C_1
\end{equation}

Ridge regression: there’s a way to get to this problem, using the constraints that can be solved efficiently numerically.

Or $\min_\beta l(\beta)$, given $||\beta||_1 \leq C_2$. This is called the Lasso regression.

In the ridge problem, we assume that the problem is sparse: we only need a few parameters to capture the relationship.

Why did we start to write it this way?

\begin{equation}
\tilde{l}(\beta) = (Y -X\beta)^T (Y-X\beta) + \lambda \beta^T \beta
\end{equation}

\begin{equation}
\frac{\partial \tilde{l}(\beta)}{\partial \beta} = 2 (-X^T(Y-X\beta) + \lambda \beta) = 2 (-X^T Y + (X^TX + \lambda \mathcal{1})\beta
\end{equation}

$\mathcal{1}$ is the identity matrix.

What is doing is putting constraints. You add constraints and then you can solve mathematically the problem.

On wenesday, we will compare those two regression, and define the framework for machine learning.

\begin{equation}
C_{jk} \hfill p>N
\end{equation}
\end{equation}
C_{jk} = \frac{1}{N} \sum_{i=1}^N x_{ij}x_{ik}
\end{equation}

\begin{equation}
\bar{x} = 0 ; C = X^TX
\end{equation}

\begin{equation}
X_{ij} \hfill  Nxp
\end{equation}

$p>N => C$ is non invertible.\\
$N = 1, C_{jk} = x_{1j} x_{1k}$\\
$C = XX^T$ \\
here, $C$ is of rank 1.

NB : remind that $Z^TZ = ||Z||^2$
\begin{equation}
Z^T y = <Z,y> = \vec{Z}\cdot\vec{y}
\end{equation}

Mathematically, $rank(C)\leq N$.
I have too many parametetrs, not enough samples. When I try to solve this linear regression problem, I have too many solutions.

There are issues when p>N, but also when they are of the same order of magnitude.

Example: financial data.

We want to get information from this data, but there’s no label, no y data.
In general, we don’t take the raw data, but try to find something more adapted to the problem.
Here, we want to get rid of the $\alpha$ parameter; for this reason, we use the $r_{ti}$ data instead of the $s_i(t)$ parameter.

Then we define the $x_{ti}$, by substracting the mean and normalising with the standard deviation.

Therefore, the $x_{ti}$ value has a null mean, and a standard deviation rescaled to 1. Therefore each data vary within the same range.

If we move to the data $C_{ij}$

When we have some data, an important step is to watch it by eye, and try to find correlations.
If something is obvious to the eye, we’ll try to interpret it with the math. Example: Exxon&Chevron are strongly correlated, JP Morgan and Bank of America are also strongly correlated.
Thus, we may suppose that $C_{Ex,Ch} > C_{Ex,JP}$.

In order to analyse the data, we may compute the spectrum. Or see clearly from the definition that the matrix is symmetric, and has all the properties to be diagonalised.

\begin{equation}
C_{jk}, C_{jj} = 1 ; C_{ij} = C_{ji}
\end{equation}

Thus we get $\lambda_1, … , \lambda_p$ eigenvalues, and $v_1, … , v_p$ eigenvectors.

\begin{marginfigure}
\TODO
\caption{dispersion of the eigenvalues}
\label{fig2}
\end{marginfigure}

There’s no way that I can have a good estimation of these metrics.

$Bottom = control$. They just shuffle the data, make permutation of the values. It is the same stocks. Randomy shuffle the data, to remove all the interesting information (correlation between the different stocks).

It’s a way to see what kind of correlation we can get just from randomness, in a case where there’s no corraltion from the data.

We can quantify the quantity of noise.

98\% of the eigenvalues are contained in the 1st part of the data. That is 98\% of noise.

Therefore I write:
\begin{equation}
C = \sum_{j=1}^p \lafbda_j v_j v_j^T
\end{equation}
\begin{equation}
v_j^T v_k = \delta_j^k
\end{equation}

Random metrics theory: branch of statustical physics, we can compute analytically the shape of the control series. RMT.

$min_x l(x)$ given $g(x)\leq C$\\
$min_x l(x) + \lambda g(x)$  ; $\lambda \geq 0$.

\begin{marginfigure}
\TODO
\caption{scheme}
\label{fig3}
\end{marginfigure}


I assume everything is differentiable.
\begin{equation}
\Nabla_x l(x) = -\lambda \Nabla_x g(x)
\end{equation}

The conditions tells that the two gradients should be aligned, and directed in opposite directions.
Generally, it would depend on the value C.
Minimum value., can be a local minimum if the problem isn’t perfectly shaped.

If I have $l(\beta)$, and am considering the norm of $\beta$ to be less that C value:
\begin{equation}
||\beta||_q \leq C_q
\end{equation}
We can do it with $l_0$, $l_2$, sometimes also with the $l_1$ norm.

Why are we interested in that kind of things, what does it give us?

With a not too big dataset, taken from a book. The goal here is to try to predict the crime rate, and to what it is correlated.

$(x_{ij}, y_i)$ $i=1..N=50$ cities
$j=1…5$

The idea is to consider naively a simple problem.

Here we may find linear combination of all different problems. For a physicist, it seems we’re not allowed to to so, because not homogeneous. But this helps finding correlations.

$x’_{ij} = x_{ij} - \bar{x_j}$. In this case, we have zero mean, We may also want try to divide by the standard deviation. Not the case here.

\begin{equation}
y=\sum_{j=1}^p \beta_j x_j
\end{equation}

Therefore we can write $l(\beta) = \frac{1}{N} \sum_{i=1}^N (y_i - \sum_{j=1}^p \beta_j x_{ij})^2$

The result of this optimismation may be given as a function.

Graph on the right, ridge regression.
What is plotted is the value of the $\beta$ along the $x$ axis. This has to do with the cost ($c_q$). 
We can repeat for different values of the cost.
If we do it for large $C$, we do not put any constraints, and therefore get the same $\beta$.

if we put a very strong cost, like 0, the only solution is $\beta = 0$.
Hence we’re looking at different solutions, constrained, and then we relax it to a state where there’s no constraint anymore.

The lasso graph is the same, but performed with $l_1$ norm.

There’s a way to understand this, by giving an illustration.

Fig 2.2 slides.


\begin{equation}
y=F(x,\theta).
\end{equation}

Linear models:
\begin{equation}
F(x,\theta) = \sum_{j=1}^p \theta_j x_j
\end{equation}

The principle is to have the results of p data, and then once we get another dataset, similar to the previous one, we’re able to fit it and to find the solution.
\begin{equation}
x_1, \cdots x_p, x_1^2, \cdots, x_p^2, \cdots cos x_1, \cdots
\end{equation}

\begin{equation}
F(x, \theta) = \sum \theta_j h_j(x)
\end{equation}

Later on, we’lll see neural networks. There, the function h people are generally using is:
$h_j(x) = \frac{1}{1 + \exp (-\omega_j^T x )} \omega_j$ is the weight. We’ll see this later.

-> There’s a relation between x and y.
There’s no limit to the complexity of the problem we can take.

Loss function $L (y, F(x, \theta)) = (y-F(x,\theta))^2$.

Training error: $err_{training}$ given a model and given a loss function:
\begin{equation}
err_{training} = \frac{1}{N} \sum_{i=1}^N L (y_i, F(x_i,\theta))
\end{equation}

This is not the only quantity we want to consider.
test/generalisation error. This one would be the error we get when we are using these datapoint that have not been used in the training of the problem.

There’s the training set, used to learn the parameters, and the additional data, on which we’re going to apply the model, and to try to generalise the data that have been used as an input for the fit.

\begin{equation}
Err_{test} = \frac{1}{N’} \sum_{i=1}^{N’} L(y_i’,F(x_i’,\theta))
\end{equation}

Plot in slides: training vs test errors.

The objective is not to get the best fit, but to generalise the given data.

procedure: K-fold cross-validation. 
We would divide the data in 5 datasets, and then, take 1 out to use as the test, and all the other one as training test. 
And then we repeat for all the other combinations. Divide data in K=5 or 10, use most of the data to train, and use one set to test.

Data set is splitted in:
\begin{itemize}
\item training set (for the fit)
\item test set (for model selection)
\item Validation set (for assessment)
\end{itemize}

Typical number would be : 50\%,25\%,25\%.

We use this to understand ho to find the best parameters.

$y=F(x)$

Training set: $\hat{\theta}$

Model $y=F(x,\theta)$

Thus we have $y= \hat{F}(x) = F(x,\theta)$

Another dataset:

$x_0$
\begin{equation}
\mathsrc{E}[(F(x_0) - \hat{F}(x_0) )^2]
\end{equation}
Where $\hat(F)$ is the prediction.

This is considered over different training sets. It says how far I am from the value I want to get the prediction.

\begin{equation}
\mathscr{E}[] = F(x_0)^2 - 2F(x_0) \mathsrc{E} (\hat{F}(x_0)) + \mathsrc{E} (\hat{F}(x_0)^2)
\end{equation}
Where $\mathsrc{E} (\hat{F}(x_0)^2)$ is $var(\hat{F}(x_0) + (\mathsrc{E}[\hat{F}(x_0)])^2$

i.e. $\mathscr{E}[] = (F(x_0) - \mathsrc{E}[\hat{F}(x_0)]^2 + var(\hat{F}(x_0))$.

Test error = $(bias)^2$ + variance

We can generalise this when there is some noise $\epsilon$ (random variable) :
\begin{equation}
y = F(x) + \epsilon
\end{equation}
There we will add another parameter : $var(y)$ that is irreducible error.

In the context of linear regression, there’s the Gauss-Markov theorem.
It tells us that in linear regression, all the estimators that have no bias, the best one is the one that is minimising the loss function
\begin{equation}
L(y,F(x)) = (y-F(x))^2
\end{equation}
This theorem is telling us that if we’re interesting in minimising the bias in the context of linear regression we should take $min_\beta l(\beta)$

No bias + min var => $min_\beta l(\beta)$.
In general, the best solution is not the solution that has no bias.
I will estimate better the parameters that I have with a constrained set of data.


\chapter{General principles}
\label{ch:general-principles}

Models, bias vs variance, cross-validation, maximum likelihood, Bayes, etc.

\chapter{Supervised learning: classificaton, regression, nearest neighbours}
\label{ch:supervised-learning}

\chapter{Unsupervised learning: dimensionality reduction, PCA, SVD}
\label{ch:unsupervised-1}

\chapter{Unsupervised learning: clustering, K-means, hierarchical}
\label{ch:unsupervised-2}

\chapter{Neural networks, from single neuron to multilayer networks}
\label{ch:neural-networks}

\chapter{Physics of machine learning, statistical mechanics of machine learning, applications}
\label{ch:physics}

\end{document}
