\documentclass[a4paper]{tufte-book}
\usepackage[utf8]{inputenc}
\hypersetup{colorlinks}% uncomment this line if you prefer colored hyperlinks (e.g., for onscreen viewing)

\title{Big Data, Statistical Learning}
\author{Julien Barrier}
\publisher{ESPCI Paris}
\newcommand{\thetitle}{Introduction to Big Data}
\newcommand{\theauthor}{Julien Barrier --- class of 2018}
\newcommand{\pc}{ESPCI Paris}
\newcommand{\thesubtitle}{Statistical \& Machine Learning}

\usepackage{microtype}
\usepackage{textcase}
\usepackage{booktabs}
\usepackage{tabularx}
\newcolumntype{R}{>{\raggedleft\arraybackslash}X}

\usepackage{graphicx}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
\graphicspath{{graphics/}}

\usepackage{fancyvrb}
\fvset{fontsize=\normalsize}

\newcommand{\hangp}[1]{\makebox[0pt][r]{(}#1\makebox[0pt][l]{)}}

\newcommand{\hangstar}{\makebox[0pt][l]{*}}

\usepackage{xspace}

\newcommand{\monthyear}{%
\ifcase\month\or January\or February\or March\or April\or May\or June\or
July\or August\or September\or October\or November\or
December\fi\space\number\year
}

\newcommand{\openepigraph}[2]{%
%\sffamily\fontsize{14}{16}\selectfont
\begin{fullwidth}
\sffamily\large
\begin{doublespace}
\noindent\allcaps{#1}\\% epigraph
\noindent\allcaps{#2}% author
\end{doublespace}
\end{fullwidth}
}

\newcommand{\blankpage}{\newpage\hbox{}\thispagestyle{empty}\newpage}

\usepackage{units}
\usepackage{siunitx}
\usepackage{stmaryrd}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{mathrsfs}

\newcommand{\measure}[3]{#1/#2$\times$\unit[#3]{pc}}

\newcommand{\hlred}[1]{\textcolor{Maroon}{#1}}% prints in red
\newcommand{\hangleft}[1]{\makebox[0pt][r]{#1}}
\newcommand{\hairsp}{\hspace{1pt}}% hair space
\newcommand{\hquad}{\hskip0.5em\relax}% half quad space
\newcommand{\TODO}{\textcolor{red}{\bf TODO!}\xspace}
\newcommand{\ie}{\textit{i.\hairsp{}e.}\xspace}
\newcommand{\eg}{\textit{e.\hairsp{}g.}\xspace}
\newcommand{\na}{\quad--}% used in tables for N/A cells
\newcommand{\E}{\mathrm{E}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{Cov}}

% Generates the index
\usepackage{makeidx}
\makeindex

\usepackage{titlesec,titletoc}
\usepackage{multirow}

\begin{document}
\frontmatter

\thispagestyle{empty}
\begin{fullwidth}
    \setlength{\parindent}{0pt}
    \begin{center}
        \fontsize{24}{24}\selectfont\textit{
            \includegraphics*[width=2.6in]{ESPCI_baseline_couleur}
        }
    \end{center}
    \vspace{3in}\fontsize{36}{54}\selectfont\thetitle

    \vspace{0.125in}\fontsize{18}{18}\selectfont\thesubtitle

    \vfill\fontsize{14}{14}\selectfont\textit{\theauthor}
\end{fullwidth}

\newpage

\cleardoublepage
\chapter*{Introduction}

I have started writing this handout from the notes I have taken from Olivier Rivoire's course on
Big Data and Statistical learning at \pc{} from May to April 2018. 
Please be considerate if some mistakes crop up in this work.

\emph{Julien}
\vspace{1cm}

Some book reading is advised during the course, particularly:

\begin{itemize}
    \item \emph{The Elements of Statistical Learning}, T.~Hastie, R.~Tibshirani and J.~Friedman, Springer Series in Statistics, 2008;
    \item \emph{Information Theory, Inference, and Learning Algorithms}, D.J.C.~MacKay, Cambridge University Press, 2003.
\end{itemize}

\vspace{1cm}

\textbf{Dr Olivier Rivoire}\\
Center for Interdisciplinary Research in Biology (CIRB)\\
Collège de France\\
olivier.rivoire@college-de-france.fr\\

\section*{Applications}

There are plenty of applications for Big Data problems. A few examples may be given:
\begin{description}
    \item[Post] learn + identify digits on enveloppes
    \item[Biology] DNA sequencing
    \item[IT] Face recognition
    \item[etc.]
\end{description}

Big Data is an issue of growing importance. As engineers, we may be familiar with such concepts.
\section*{Idea of marchine learning} 

The main idea of machine learning is to find models to give prediction of input data.
In facts, Big Data models are deduced from a training batch of N input-output
data, on which programs train to generalise models.
The deduced model \emph{input i} $\rightarrow$ \emph{output i} can then be
generalised to give prediction from a random input, as long as it relates to 
the training batch.



Analyticall, let's start with a collection of $x$ and $y$ data, where $x$ stands
for the input data and $y$ is the vector of the output data.
Each sample is going to have multiple dimensions, therefore we may use an
algebraic model. Let $N$ be the number of samples used and p the dimension of
each $x$ data. We may write x as an $N,p$ matrix and $y$ as a vector of $p$
dimensions.

We now N samples of p dimensions $x_{ij}$ associated with the N output data $y_i$.

From now on there are two possible cases: $y_i$ can be known or unknown.
In the fist case ($y_i$ known), the problem is said to be \emph{supervised}.
Hence we may work with a finite discrete set of data: $y_i = 1, \cdots, K$.
This problem is called categorical, and we can solve it with 
\emph{classification}.
We may also work with an infinite set of numbers: $y_i \in \mathbb{R}$. This
problem is called quantitative, and we can solve it with \emph{regression}.

The second case ($y_i$ unknown) is said to be \emph{unsupervised} and can be
solved via \emph{clustering} or \emph{dimension reduction} methods.

\section*{Deep learning}

In the past few years, there have been huge progress in the \emph{deep learning}
approach. It is based on so-called neurol networks, that are models inspired
by the brain operation.

People are trying to understand how to train these networks. It has had
remarkable outcomes in image recognition, social network filtering, medical
diagnoses, etc.

Deep learning is based on hidden layers, placed inbetween input and output layers
, that are trained to find correlations and mathematical models.

The goal of this course is to explain whate these objects are, how do they work,
and put it in relation with state of the art research.

What kind of open problems are there? How do neural networks operate? What are
their unsuperised learning behaviour?


\tableofcontents\thispagestyle{empty}

\mainmatter

\chapter{Least square regression, from small to big data}
\label{ch:least-square}

\section{Linear Regression at One Dimension}

Let $p =1$. If we work with N points, then $i=1,\cdots,N$, and we work with a
set of data $(x_i,y_i)$.

The goal here is to make a prediction of what the $y$ data should be when $x$ is
given.

The simplest possible model is the linear regression given by the equation 
\ref{linear1D}.
\begin{equation}
    y=\alpha + \beta x.
    \label{linear1D}
\end{equation}
Here, the main issue is to get the best $\alpha$ and $\beta$ for a particular set
of data. To know what the best choice is, we may define a cost function, that
returns a number representing how well the regression permorms. In neural network
problems, the cost fuction return number is associated with how well the neural
network performs in mapping training examples to the correct output.

There are several choices that can be made to define the cost function. At one
dimension, the simplest choice is the sum of squared residuals, defined in
equation \ref{SSR}, usually shortened as SSR.

\begin{equation}
    l(\alpha,\beta) = \frac{1}{N} \sum_{i=1}^N (y_i - \alpha - \beta x_i )^2
    \label{SSR}
\end{equation} 

Figure \ref{fig1} illustrate a simple geometrical interpretation of what the SSR
is. Actually, the lower $\epsilon_i$, the better the fit.

\begin{marginfigure}
\TODO
\caption{geometrical intepretation of the SSR, where $\epsilon_i$ is given by the relation: $\epsilon_i^2 = (y_i - \alpha - \beta x_i)^2$}
\label{fig1}
\end{marginfigure}

For all we have done up to now, we never have never worked with big data. We
need $p$ large enough to consider this as a real big data issue.

If we’re looking at a hundreds or thousands pixels picture composed of hundreds, 
$p$ will be large in comparison with $N$. That is a full statistics problem.

Currently, $p=1$ is small data, but all we did there has been a correct
introduction to clearly understand big data problems.

Striking a good fit necessitates finding the best $\alpha$ and the best $\beta$.
For this, we may look at the optimum, defined as the points where the derivative
of $l$ versus $\alpha$ and $\beta$ vanishes. This is given by equations
\ref{derlalpha} and \ref{derlbeta}.

\begin{eqnarray}
    \frac{\partial l}{\partial \alpha} & = & - \frac{1}{N} \sum_{i=1}^N (y_i - \alpha - \beta x_i) = 0
    \label{derlalpha}\\
    \frac{\partial l}{\partial \beta} & =&  -\frac{1}{N} \sum_{i=1}^N x_i (y_i - \alpha -  \beta x_i) = 0
    \label{derlbeta}
\end{eqnarray}

To solve this set of equations, we require a substitution for $x$ and $y$.

Let’s define the mean values\footnote{We must keep in mind that $\overline{x^2} \neq \bar{x}^2$.}:
\begin{eqnarray}
    \bar{x} & = & \frac{1}{N} \sum_{i=1}^N x_i\\
    \bar{y} & = & \frac{1}{N} \sum_{i=1}^N y_i\\
    \overline{xy} & =&  \frac{1}{N} \sum_{i=1}^{N} x_i y_i\\
    \overline{x^2} & =&  \frac{1}{N} \sum_{i=1}^N x_i^2
\end{eqnarray}

Thus, equations \ref{derlalpha} and \ref{derlbeta} can be reduced as:
\begin{eqnarray}
    \frac{\partial l}{\partial \alpha} &=& - (\bar{y} - \alpha - \beta \bar{x})
    \label{deralpha2}\\
    \frac{\partial l}{\partial \beta} &=& - (\overline{xy} - \alpha \bar{x} - \beta \overline{x^2})
    \label{derbeta2}
\end{eqnarray}

This yields to:

\begin{eqnarray}
    \alpha & = & \bar{y} + \beta \bar{x}\\
    \overline{xy} & = &  -\bar{y}\bar{x} - \beta \bar{x}^2 - \beta \overline{x^2}
\end{eqnarray}

\marginnote{We use the given notations:
    \begin{eqnarray*}
        \cov(x,y) &=& \bar{xy} - \bar{x}\bar{y} \\
        & =& \overline{(x-\bar{x})}(y-\bar{y}) \\
        \var(x) & =&  \cov (x,x) \\
        \sigma(x) & =& \sqrt{\var(x)}
    \end{eqnarray*}
}

There we may substitute $\alpha$ and $\beta$:

\begin{eqnarray*}
    \hat{\alpha} & =& \bar{y} - \hat{\beta}\bar{x}
\end{eqnarray*}

\begin{eqnarray*}
    \hat{\beta} & = & \frac{\overline{xy} - \bar{x}\bar{y}}{\overline{x^2} - \bar{x}^2} \\
    & = & \frac{\cov(x,y)}{\cov(x,x)}\\
    & =&  \frac{\cov(x,y)}{\var(x)}
\end{eqnarray*}


Let us define the Pearson coefficient $\mathcal{R}$ by the relation \ref{pearson}.
\begin{equation}
    \mathcal{R} = \frac{\cov(x,y)}{\sigma(x) \sigma(y)}
    \label{pearson}
\end{equation}

The pearson coefficient $\mathcal{R}$ is always comprised between $0$ and $1$.
Thus, we can define the quantity $\mathcal{R}^2$, that relates to the quality of
the fit:

\begin{equation}
    \mathcal{R}^2 = 1 - \frac{\hat{l}}{\var(y)}
\end{equation}


In the general case, we look at models where $\beta = 0$. In this case, the cost
function $l$ would be the sum of the square distance to the line.
Figure \ref{squaredistances} depicts two linear regressions with different
parameters. The right figure shows a much better linear regression, with
much lower square distances between the points and the line.

\begin{figure}
    \includegraphics{./Figures/squaredistances.png}
    \caption{Two linear regression taken from two datasets. The left one shows higher square distances than the right one.}
    \label{squaredistances}
\end{figure}

We may note that in some cases, it would be better to rescale the axis to
fit the data with a linear regression. Logarithm axis is the most popular way
of rescaling an axis to have a correct assumption. It is usually more valuable
to rescale the axis and perform a linear regression, than trying to find an
higher order fit.

\section{Linear Regression at Higher Dimensions}

We are now considering higher dimensions data ($p>1$), that are full, meaning
that $p<<N$.
It means that, for an example of data, we might add different parameters. If we
take the example (given in class) of correlations between the velocity of people
versus the size of towns, we might add other relevant parameters, like the
average heigh of people, their ages, etc. We might then examine many potential
predictors. Thus we need to generalise the same things, where each input now
becomes a vector of p dimensions, as presented in equation \ref{pdim}

\begin{equation}
    (x_{i,1}, \cdots , x_{i,p}), \qquad \forall i
    \label{pdim}
\end{equation}

Let now $x_{i,j}$ be the matrix of the input data, where $i$ is the number of
samples, varying from 1 to $N$, and $j$ is the dimension, ranged between 1 to $p$
.

We may generalise the relation $\hat{y_i} = \hat{\alpha} + \hat{\beta}x_i$ in the
new \ref{gen2} equation: 

\begin{equation} 
    \hat{y_i} = \hat{\alpha} + \sum_{j=1}^p \hat{\beta_j} x_{ij}
    \label{gen2}
\end{equation}

If we have this key figure, we can always add 1 in the x vector, as the $p+1$
coordinate. We can thus assume that $\alpha$ vanishes. In fact, we can always
redefine the data so that $\alpha$ vanishes. We can also rescale the variable, by
removing the mean:

\begin{eqnarray}
x’&=& x-\bar{x}\\
y’&=& y-\bar{y}
\end{eqnarray}

Therefore, the output coordinate $\hat{y_i}$ can be written as the product
$\hat{y_i} = X \hat{\beta}$, that is a much more convenient way to write it.

That are just restrictions of the problems that help us to compute it.

Let $l(\beta)$ be the cross-function, define with equation \ref{lbeta}.

\begin{equation}
    l(\beta) = \frac{1}{N} \sum_{i=1}^N \left( y_i - \sum_{j=1}^p \beta_j x_{ij} \right)^2
    \label{lbeta}
\end{equation}

Let $Z$ be a vector whose components $z_i$ are defined as follows:

\begin{equation}
    \sum_{i=1}^N z_i^2 = ||Z||^2 = Z^TZ
\end{equation}

Therefore we can write the cross-function as:

\begin{equation}
    l(\beta) = \frac{1}{N} (Y-X\beta)^T(Y-X\beta)
\end{equation}

This form can easily be differentiated with $\beta$, and the retrieved derivative
vanishes at the extremum (eq. \ref{lextrem}).

\begin{equation}
    \frac{\partial l}{\partial \beta} =  -\frac{Z}{N} X^T(Y-X\beta) =0
    \label{lextrem}
\end{equation}

\marginnote{$^T$ denotes the transpose matrix, defined by the relation:
$\left[\mathbf{A}^\mathrm{T}\right]_{ij} = \left[\mathbf{A}\right]_{ji}$}

The equation \ref{lextrem} can be reduced as $X^TY = X^TX\beta$, which can be
solved by introducing the matrix $C = X^TX$ (eq. \ref{lextremC})\footnote{Note
    that 
\begin{equation*}
C_{ij} = \sum_{k=1}^N x_{kj} x_{ki}
\end{equation*}}

\begin{equation}
    \hat{\beta} = (X^TX)^{-1} X^TY = C^{-1} X^TY
    \label{lextremC}
\end{equation}

At higher dimension, the geometry consists in fitting with an hyperplane, as
shown on figure \ref{hyperplane}.

\begin{marginfigure}
    \includegraphics{./Figures/hyperplane.png}
    \caption{Linear least squares fitting with $X\in\mathbb{R}^2$. In this
        problem, we are looking for the linear function of X that minimises the
    sum of squared residuals from Y, which is an plane (hyperplane in dim 3)}
    \label{hyperplane}
\end{marginfigure}

Here, we are essentially solving a system of equations. We must consider the
number of variables adapted to the number of equations that we get. When there is
not enough equations (when $p$ is too small for example), the system is
undetermined. We cannot reduce it and do not have a single solution.

Actually, when $p>N$, we can solve this problem with the condition $\hat{l}=0$.
This is a situation where there are more parameters than there are equaltions. It
is easy to solve. The solutions consists in overfitting.

For instance if we have 100 parameters and 10 equations, we can never manage to
get any result. However, we can find easy solutions, but this will overfit.
At this stage, the system cannot be inverted.

When $p$ is large, even if it is of the same order of magnitude as $N$, we are
working with big matrixes, that can be tricky to invert with both proper
mathematical accuracy and computation performances. In such cases, we should
handle the data carefuly.

Large $p$ are typical way of using statistical learning methods, by
discriminating between datasets without having the aforesaid issues.


\chapter{General principles}
\label{ch:general-principles}

Models, bias vs variance, cross-validation, maximum likelihood, Bayes, etc.


So, what do we do now?
In general, we want to know what are the most interesting parameters. At the end, we can predict the issue with one or two parameters. Even if we start with a lot of data, we want to find what are the parameterts, the dimensions important in the problem we are working on.

Two advantages:
\begin{description}
\item[Interpretation]
With less parameters, we can estimate them with much more accuracy than if we have more.
In general, it’s easier and more accurate to estimate things on a condensed set of parameters than on a large set.
The issue is: how do you compromise: having enough parameters to have a good enough estimation of the problem, without having too much and loosing precision. Enough information VS enough precision.
\end{description}

Let $\tilde{l}(\beta)$ be defined as:
\begin{equation}
\tilde{l}(\beta) = l(\beta) + \lambda ||\beta||_q
\end{equation}
With $||\beta||_0 = \#(\beta_j \neq 0)$ (cardinal)

We look for $min_\beta l(\beta)$ given $||\beta||_0 \leq C$

With $\beta = [0, \cdots, \beta_i, 0, \cdots, 0]$
There ain’t any good numerical solution.

There’s always a compromise between what we are able to optimise efficiently, and what is possible to optimise. A version of the problem that is easy to solve is:

\begin{equation}
min_\beta l(\beta)  given ||\beta||_2 \leq C_1
\end{equation}

Ridge regression: there’s a way to get to this problem, using the constraints that can be solved efficiently numerically.

Or $\min_\beta l(\beta)$, given $||\beta||_1 \leq C_2$. This is called the Lasso regression.

In the ridge problem, we assume that the problem is sparse: we only need a few parameters to capture the relationship.

Why did we start to write it this way?

\begin{equation}
\tilde{l}(\beta) = (Y -X\beta)^T (Y-X\beta) + \lambda \beta^T \beta
\end{equation}

\begin{equation}
\frac{\partial \tilde{l}(\beta)}{\partial \beta} = 2 (-X^T(Y-X\beta) + \lambda \beta) = 2 (-X^T Y + (X^TX + \lambda \mathcal{1})\beta
\end{equation}

$\mathcal{1}$ is the identity matrix.

What is doing is putting constraints. You add constraints and then you can solve mathematically the problem.

On wenesday, we will compare those two regression, and define the framework for machine learning.

\begin{equation}
C_{jk} \hfill p>N
\end{equation}
\begin{equation}
C_{jk} = \frac{1}{N} \sum_{i=1}^N x_{ij}x_{ik}
\end{equation}

\begin{equation}
\bar{x} = 0 ; C = X^TX
\end{equation}

\begin{equation}
X_{ij} \hfill  Nxp
\end{equation}

$p>N => C$ is non invertible.\\
$N = 1, C_{jk} = x_{1j} x_{1k}$\\
$C = XX^T$ \\
here, $C$ is of rank 1.

NB : remind that $Z^TZ = ||Z||^2$
\begin{equation}
Z^T y = <Z,y> = \vec{Z}\cdot\vec{y}
\end{equation}

Mathematically, $rank(C)\leq N$.
I have too many parametetrs, not enough samples. When I try to solve this linear regression problem, I have too many solutions.

There are issues when p>N, but also when they are of the same order of magnitude.

Example: financial data.

We want to get information from this data, but there’s no label, no y data.
In general, we don’t take the raw data, but try to find something more adapted to the problem.
Here, we want to get rid of the $\alpha$ parameter; for this reason, we use the $r_{ti}$ data instead of the $s_i(t)$ parameter.

Then we define the $x_{ti}$, by substracting the mean and normalising with the standard deviation.

Therefore, the $x_{ti}$ value has a null mean, and a standard deviation rescaled to 1. Therefore each data vary within the same range.

If we move to the data $C_{ij}$

When we have some data, an important step is to watch it by eye, and try to find correlations.
If something is obvious to the eye, we’ll try to interpret it with the math. Example: Exxon\&Chevron are strongly correlated, JP Morgan and Bank of America are also strongly correlated.
Thus, we may suppose that $C_{Ex,Ch} > C_{Ex,JP}$.

In order to analyse the data, we may compute the spectrum. Or see clearly from the definition that the matrix is symmetric, and has all the properties to be diagonalised.

\begin{equation}
C_{jk}, C_{jj} = 1 ; C_{ij} = C_{ji}
\end{equation}

Thus we get $\lambda_1, … , \lambda_p$ eigenvalues, and $v_1, … , v_p$ eigenvectors.

\begin{marginfigure}
\TODO
\caption{dispersion of the eigenvalues}
\label{fig2}
\end{marginfigure}

There’s no way that I can have a good estimation of these metrics.

$Bottom = control$. They just shuffle the data, make permutation of the values. It is the same stocks. Randomy shuffle the data, to remove all the interesting information (correlation between the different stocks).

It’s a way to see what kind of correlation we can get just from randomness, in a case where there’s no corraltion from the data.

We can quantify the quantity of noise.

98\% of the eigenvalues are contained in the 1st part of the data. That is 98\% of noise.

Therefore I write:
\begin{equation}
C = \sum_{j=1}^p \lambda_j v_j v_j^T
\end{equation} 
\begin{equation}
v_j^T v_k = \delta_j^k
\end{equation}

Random metrics theory: branch of statustical physics, we can compute analytically the shape of the control series. RMT.

$min_x l(x)$ given $g(x)\leq C$\\
$min_x l(x) + \lambda g(x)$  ; $\lambda \geq 0$.

\begin{marginfigure}
\TODO
\caption{scheme}
\label{fig3}
\end{marginfigure}


I assume everything is differentiable.
\begin{equation}
\nabla_x l(x) = -\lambda \nabla_x g(x)
\end{equation}

The conditions tells that the two gradients should be aligned, and directed in opposite directions.
Generally, it would depend on the value C.
Minimum value., can be a local minimum if the problem isn’t perfectly shaped.

If I have $l(\beta)$, and am considering the norm of $\beta$ to be less that C value:
\begin{equation}
||\beta||_q \leq C_q
\end{equation}
We can do it with $l_0$, $l_2$, sometimes also with the $l_1$ norm.

Why are we interested in that kind of things, what does it give us?

With a not too big dataset, taken from a book. The goal here is to try to predict the crime rate, and to what it is correlated.

$(x_{ij}, y_i)$ $i=1..N=50$ cities
$j=1…5$

The idea is to consider naively a simple problem.

Here we may find linear combination of all different problems. For a physicist, it seems we’re not allowed to to so, because not homogeneous. But this helps finding correlations.

$x’_{ij} = x_{ij} - \bar{x_j}$. In this case, we have zero mean, We may also want try to divide by the standard deviation. Not the case here.

\begin{equation}
y=\sum_{j=1}^p \beta_j x_j
\end{equation}

Therefore we can write $l(\beta) = \frac{1}{N} \sum_{i=1}^N (y_i - \sum_{j=1}^p \beta_j x_{ij})^2$

The result of this optimismation may be given as a function.

Graph on the right, ridge regression.
What is plotted is the value of the $\beta$ along the $x$ axis. This has to do with the cost ($c_q$). 
We can repeat for different values of the cost.
If we do it for large $C$, we do not put any constraints, and therefore get the same $\beta$.

if we put a very strong cost, like 0, the only solution is $\beta = 0$.
Hence we’re looking at different solutions, constrained, and then we relax it to a state where there’s no constraint anymore.

The lasso graph is the same, but performed with $l_1$ norm.

There’s a way to understand this, by giving an illustration.

Fig 2.2 slides.


\begin{equation}
y=F(x,\theta).
\end{equation}

Linear models:
\begin{equation}
F(x,\theta) = \sum_{j=1}^p \theta_j x_j
\end{equation}

The principle is to have the results of p data, and then once we get another dataset, similar to the previous one, we’re able to fit it and to find the solution.
\begin{equation}
x_1, \cdots x_p, x_1^2, \cdots, x_p^2, \cdots cos x_1, \cdots
\end{equation}

\begin{equation}
F(x, \theta) = \sum \theta_j h_j(x)
\end{equation}

Later on, we’lll see neural networks. There, the function h people are generally using is:
$h_j(x) = \frac{1}{1 + \exp (-\omega_j^T x )} \omega_j$ is the weight. We’ll see this later.

-> There’s a relation between x and y.
There’s no limit to the complexity of the problem we can take.

Loss function $L (y, F(x, \theta)) = (y-F(x,\theta))^2$.

Training error: $err_{training}$ given a model and given a loss function:
\begin{equation}
err_{training} = \frac{1}{N} \sum_{i=1}^N L (y_i, F(x_i,\theta))
\end{equation}

This is not the only quantity we want to consider.
test/generalisation error. This one would be the error we get when we are using these datapoint that have not been used in the training of the problem.

There’s the training set, used to learn the parameters, and the additional data, on which we’re going to apply the model, and to try to generalise the data that have been used as an input for the fit.

\begin{equation}
Err_{test} = \frac{1}{N’} \sum_{i=1}^{N’} L(y_i’,F(x_i’,\theta))
\end{equation}

Plot in slides: training vs test errors.

The objective is not to get the best fit, but to generalise the given data.

procedure: K-fold cross-validation. 
We would divide the data in 5 datasets, and then, take 1 out to use as the test, and all the other one as training test. 
And then we repeat for all the other combinations. Divide data in K=5 or 10, use most of the data to train, and use one set to test.

Data set is splitted in:
\begin{itemize}
\item training set (for the fit)
\item test set (for model selection)
\item Validation set (for assessment)
\end{itemize}

Typical number would be : 50\%,25\%,25\%.

We use this to understand ho to find the best parameters.

$y=F(x)$

Training set: $\hat{\theta}$

Model $y=F(x,\theta)$

Thus we have $y= \hat{F}(x) = F(x,\theta)$

Another dataset:

$x_0$
\begin{equation}
\E [(F(x_0) - \hat{F}(x_0) )^2]
\end{equation}

Where $\hat(F)$ is the prediction.

This is considered over different training sets. It says how far I am from the value I want to get the prediction.

\begin{equation}
\E[] = F(x_0)^2 - 2F(x_0) \E (\hat{F}(x_0)) + \E (\hat{F}(x_0)^2)
\end{equation}
Where $\E (\hat{F}(x_0)^2)$ is $\var(\hat{F}(x_0) + (\E[\hat{F}(x_0)])^2$

i.e. $\E[] = (F(x_0) - \E[\hat{F}(x_0)]^2 + \var(\hat{F}(x_0))$.

Test error = $(bias)^2$ + variance

We can generalise this when there is some noise $\epsilon$ (random variable) :
\begin{equation}
y = F(x) + \epsilon
\end{equation} 
There we will add another parameter : $var(y)$ that is irreducible error.

In the context of linear regression, there’s the Gauss-Markov theorem.
It tells us that in linear regression, all the estimators that have no bias, the best one is the one that is minimising the loss function
\begin{equation}
L(y,F(x)) = (y-F(x))^2
\end{equation}
This theorem is telling us that if we’re interesting in minimising the bias in the context of linear regression we should take $min_\beta l(\beta)$

No bias + min var => $min_\beta l(\beta)$.
In general, the best solution is not the solution that has no bias.
I will estimate better the parameters that I have with a constrained set of data.


\chapter{Supervised learning: classificaton, regression, nearest neighbours}
\label{ch:supervised-learning}

\chapter{Unsupervised learning: dimensionality reduction, PCA, SVD}
\label{ch:unsupervised-1}

\chapter{Unsupervised learning: clustering, K-means, hierarchical}
\label{ch:unsupervised-2}

\chapter{Neural networks, from single neuron to multilayer networks}
\label{ch:neural-networks}

\chapter{Physics of machine learning, statistical mechanics of machine learning, applications}
\label{ch:physics}

\end{document}
