\documentclass[a4paper]{tufte-book}
\usepackage[utf8]{inputenc}
\hypersetup{colorlinks}% uncomment this line if you prefer colored hyperlinks (e.g., for onscreen viewing)

\title{Big Data, Statistical Learning}
\author{Julien Barrier}
\publisher{ESPCI Paris}
\newcommand{\thetitle}{Introduction to Big Data}
\newcommand{\theauthor}{Julien Barrier --- class of 2018}
\newcommand{\pc}{ESPCI Paris}
\newcommand{\thesubtitle}{Statistical \& Machine Learning}

\usepackage{hyperref}
\usepackage{microtype}
\usepackage{textcase}
\usepackage{booktabs}
\usepackage{tabularx}
\newcolumntype{R}{>{\raggedleft\arraybackslash}X}

\usepackage{graphicx}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
\graphicspath{{graphics/}}

\usepackage{fancyvrb}
\fvset{fontsize=\normalsize}

\newcommand{\hangp}[1]{\makebox[0pt][r]{(}#1\makebox[0pt][l]{)}}

\newcommand{\hangstar}{\makebox[0pt][l]{*}}

\usepackage{xspace}

\newcommand{\monthyear}{%
\ifcase\month\or January\or February\or March\or April\or May\or June\or
July\or August\or September\or October\or November\or
December\fi\space\number\year
}

\newcommand{\openepigraph}[2]{%
%\sffamily\fontsize{14}{16}\selectfont
\begin{fullwidth}
\sffamily\large
\begin{doublespace}
\noindent\allcaps{#1}\\% epigraph
\noindent\allcaps{#2}% author
\end{doublespace}
\end{fullwidth}
}

\newcommand{\blankpage}{\newpage\hbox{}\thispagestyle{empty}\newpage}

\usepackage{units}
\usepackage{siunitx}
\usepackage{stmaryrd}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{mathrsfs}
\usepackage{dsfont}
\newcommand{\measure}[3]{#1/#2$\times$\unit[#3]{pc}}
\DeclareMathOperator{\sign}{sign}

\newcommand{\hlred}[1]{\textcolor{Maroon}{#1}}% prints in red
\newcommand{\hangleft}[1]{\makebox[0pt][r]{#1}}
\newcommand{\hairsp}{\hspace{1pt}}% hair space
\newcommand{\hquad}{\hskip0.5em\relax}% half quad space
\newcommand{\TODO}{\textcolor{red}{\bf TODO!}\xspace}
\newcommand{\ie}{\textit{i.\hairsp{}e.}\xspace}
\newcommand{\eg}{\textit{e.\hairsp{}g.}\xspace}
\newcommand{\na}{\quad--}% used in tables for N/A cells
\newcommand{\E}{\mathrm{E}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\nth}{\frac{1}{N}}
\newcommand{\sumin}{\sum_{i=1}^N}

% Generates the index
\usepackage{makeidx}
\makeindex

\usepackage{titlesec,titletoc}
\usepackage{multirow}

\begin{document}
\frontmatter

\thispagestyle{empty}
\begin{fullwidth}
    \setlength{\parindent}{0pt}
    \begin{center}
        \fontsize{24}{24}\selectfont\textit{
            \includegraphics*[width=2.6in]{ESPCI_baseline_couleur}
        }
    \end{center}
    \vspace{3in}\fontsize{36}{54}\selectfont\thetitle

    \vspace{0.125in}\fontsize{18}{18}\selectfont\thesubtitle

    \vfill\fontsize{14}{14}\selectfont\textit{\theauthor}
\end{fullwidth}

\newpage

\cleardoublepage
\chapter*{Introduction}

I have started writing this handout from the notes I have taken from
Olivier Rivoire's course on Big Data and Statistical learning at \pc{} from
March to April 2018. The \LaTeX ~sources of this document can be found on
\href{https://github.com/julienbarrier/bigdata-handout}{Github}. Do not hesitate
to update it if you feel it necessary.

Please be considerate if some mistakes crop up in this work.

\vspace{.5cm}
\emph{Julien}
\vspace{1cm}

Some book reading is advised during the course, partialicularly:

\begin{itemize}
    \item \emph{The Elements of Statistical Learning}, T.~Hastie, R.~Tibshirani and J.~Friedman, Springer Series in Statistics, 2008;
    \item \emph{Information Theory, Inference, and Learning Algorithms}, D.J.C.~MacKay, Cambridge University Press, 2003.
\end{itemize}

\vspace{1cm}

\textbf{Dr Olivier Rivoire}\\
Center for Interdisciplinary Research in Biology (CIRB)\\
Coll√®ge de France\\
olivier.rivoire@college-de-france.fr\\

\section*{Applications}

There are plenty of applications for Big Data problems. A few examples may be
given:
\begin{description}
    \item[Post] learn + identify digits on enveloppes
    \item[Biology] DNA sequencing
    \item[IT] Face recognition
    \item[etc.]
\end{description}

Big Data is an issue of growing importance. As engineers, we may be familiar 
with such concepts.
 \section*{Idea of marchine learning} 

The main idea of machine learning is to find models to give prediction of input data.
In facts, Big Data models are deduced from a training batch of N input-output
data, on which programs train to generalise models.
The deduced model \emph{input i} $\rightarrow$ \emph{output i} can then be
generalised to give prediction from a random input, as long as it relates to 
the training batch.



Analytically, let's start with a collection of $x$ and $y$ data, where $x$ 
stands for the input data and $y$ is the vector of the output data.
Each sample is going to have multiple dimensions, therefore we may use an
algebraic model. Let $N$ be the number of samples used and p the dimension of
each $x$ data. We may write x as an $N,p$ matrix and $y$ as a vector of $p$
dimensions.

We now N samples of p dimensions $x_{ij}$ associated with the N output data
$y_i$.

From now on there are two possible cases: $y_i$ can be known or unknown.
In the fist case ($y_i$ known), the problem is said to be \emph{supervised}.
Hence we may work with a finite discrete set of data: $y_i = 1, \cdots, K$.
This problem is called categorical, and we can solve it with 
\emph{classification}.
We may also work with an infinite set of numbers: $y_i \in \mathbb{R}$. This
problem is called quantitative, and we can solve it with \emph{regression}.

The second case ($y_i$ unknown) is said to be \emph{unsupervised} and can be
solved via \emph{clustering} or \emph{dimension reduction} methods.

\section*{Deep learning}

In the past few years, there have been huge progress in the \emph{deep learning}
approach. It is based on so-called neural networks, that are models inspired
by the brain operation.

People are trying to understand how to train these networks. It has had
remarkable outcomes in image recognition, social network filtering, medical
diagnoses, etc.

Deep learning is based on hidden layers, placed inbetween input and output
layers, that are trained to find correlations and mathematical models.

The goal of this course is to explain whate these objects are, how do they work,
and put it in relation with state of the art research.

What kind of open problems are there? How do neural networks operate? What are
their unsuperised learning behaviour?


\tableofcontents\thispagestyle{empty}

\mainmatter

\include{chapter/least-sqare}
\include{chapter/general-principles}
\include{chapter/supervised-learning}
\include{chapter/unsupervised-learning}
\include{chapter/neural-networks}
\include{chapter/physics}

\chapter*{Conclusion}
cambridge analytica

very simple mathematics
very powerful tool
unregulated
large scale
lots of profits that are being made.
we know a bit of those things now, that are likely to evolve. it is one of the
sectors we can find the best salaries and best opportunities as physicists.
there are many ways we can do good things and bad things
entropy principle: things will evolve wrong

even if we have good intentions, we should be aware that many things can be done
with the data. as scientists and engineers, we have to think of what could be
the implications.
this is very important to say.
most of current applications are currently in marketing, not oriented towards
science.

\end{document}
